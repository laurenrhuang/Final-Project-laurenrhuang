---
title: "Final Project"
author: "Lauren Huang"
date: "2022-10-14"
output:
  html_document:
    toc: true
    code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Predicting students' grades in two Portuguese schools

A brief introduction to the selected data set: The data contains information regarding students' academic achievement in secondary education of two Portuguese schools. There are two datasets. One for Maths class, one for Portuguese class. Both datasets contain the same predictor variables, including but not limited to: school, sex, age, address, family size, parent status, mother's education, father's education, mother's job, father's job, etc. There are also variables called G1, G2 and G3. G1 represents a student's grade during first period, G2 represents a student's grade during second period. G3 represents a student's final grade in the class. Having both G1 and G2 are extremely helpful in predicting G3, which is a student's final grade in a class.

## Reading in the data, cleaning the data

The first step is to install packages for any functions or models we want to fit. For the framework of our models, we'll be using tidymodels.

```{r, message = F, warning = F}
library(tidyverse)
library(tidymodels)
library(MASS)
library(discrim)
library(poissonreg)
library(corrr)
library(corrplot)
library(pROC) # needed for AUC
library(janitor)
library(rpart.plot)
library(vip)
library(randomForest)
library(xgboost)
tidymodels_prefer()
```

After installing necessary packages, we read in our two datasets for the classes "Maths" and "Portuguese." Before we do any analysis, we should get to know our data better first. Let's take a look at the variables, the number of observations in each dataset, and make histograms for each of the datasets to see how G3 student grades are distributed.

```{r}
# read in the Maths dataset
Maths <- read_csv("C:/Users/cupca/OneDrive/Documents/UCSB/Fall2022/PSTAT131/Final-Project/Final-Project-laurenrhuang/data/Maths.csv", show_col_types = FALSE)

# clean variable names
Maths <- clean_names(Maths)
Maths %>% head()

# number of observations in Maths
dim(Maths)

# read in the Portuguese dataset
Portuguese <- read_csv("C:/Users/cupca/OneDrive/Documents/UCSB/Fall2022/PSTAT131/Final-Project/Final-Project-laurenrhuang/data/Portuguese.csv", show_col_types = FALSE)

# clean variable names
Portuguese <- clean_names(Portuguese)
Portuguese %>% head()

# number of observations in Portuguese
dim(Portuguese)
```

```{r}
# distribution of G3 grades in Maths
hist(Maths$g3,
     breaks=19,
     xlab = "student final grades (G3) in Maths",
     ylab = "count")

# distribution of G3 grades in Portuguese
hist(Portuguese$g3,
     breaks=19,
     xlab = "student final grades (G3) in Portuguese",
     ylab = "count")
```

From the histograms, it looks like most students' G3 grades are concentrated between 5 and 20, with the most common grades being around 10. Maths G3 grades are slightly more centered, while Portuguese G3 grades are slightly right skewed. Interestingly, there are also many students whose Maths G3 grades are 0.

We can also look at make a correlation matrix to see the relationship between variables in both datasets.

```{r}
# handle non-numeric variables
Maths_numeric <- Maths %>% select(-school, -sex, -age, -address, -famsize, -pstatus, -mjob, -fjob, -reason, -guardian, -schoolsup, -famsup, -paid, -activities, -nursery, -higher, -internet, -romantic)

# Maths correlation matrix
Maths_cor = cor(Maths_numeric)
corrplot(Maths_cor)


# handle non-numeric variables
Portuguese_numeric <- Portuguese %>% select(-school, -sex, -age, -address, -famsize, -pstatus, -mjob, -fjob, -reason, -guardian, -schoolsup, -famsup, -paid, -activities, -nursery, -higher, -internet, -romantic)

# Portuguese correlation matrix
Portuguese_cor = cor(Portuguese_numeric)
corrplot(Portuguese_cor)
```

The first thing that stands out to me among the two matrices is that several of the predictors are more strongly correlated (whether negative or positive) in the Portuguese dataset than the Maths dataset. You can tell by the color and size of the dots, some of the dots in the Portuguese correlation matrix tend to be darker in color and larger in size. One example of this is relationship between G1, G2 grades and failures. Are there any other relationships that stand out to you?

As we mentioned earlier, the goal of using this data will be using our predictor variables (ex. school, sex, age, address, family size, parental status, mother's education, father's education, etc.) to predict a student's G3 (final grade) grade/whether a student will pass the class. As we look through the variables, we see that G1 and G2 grades are included. This is great because they will help us in predicting G3 grades.

Let's add a variable called "course" so we know which observations are from which dataset. This is important later when we merge and split the dataset, we will want to keep track.

```{r}
# add course = "Maths" as a predictor to label the dataset
course <- rep("maths", times=395)
Maths_new <- cbind(Maths, course)
Maths_new <- clean_names(Maths_new)
Maths_new %>% head()

# add course = "Portuguese" as a predictor to label the dataset
course <- rep("portuguese", times=649)
Portuguese_new <- cbind(Portuguese, course)
Portuguese_new <- clean_names(Portuguese_new)
Portuguese_new %>% head()
```

## Exploratory Data Analysis

### How should we use the 2 datasets?

Since we have 2 datasets and not just 1, there are more options with how we can use them. Let's try two different ways: 
  1. Method 1 will be to take both the Maths and Portuguese datasets and merge them into one big dataset. 
  2. Method 2 will be to use one dataset to train, and the other to test our models. I will be using Portuguese to train and Maths to test because Portuguese has more observations (usually we want the proportion of training data to be bigger than testing!)
We will use both methods of data splitting to see which gives us more accurate predictions from the models.

#### Method 1. Merging the data

For method 1, let's merge both Maths and Portuguese into one large dataset.  Mainly we will be working with the data that is labeled by course. After merging, split the merged dataset into training dataset and testing dataset, stratifying on G3, our outcome variable. Stratifying on G3 helps us keep the proportions of the original dataset when we split into training and testing! (You'll notice I'm also merging the data that doesn't have the course variable because I'll need to use it later on.)

```{r}
# merge dataset (without labels) into one large dataset 
all_classes <- rbind(Maths, Portuguese) # for train/testing use later on in the project
all_classes %>% head()

# merge course labeled Maths and Portuguese datasets into one large dataset
all_classes2 <- rbind(Maths_new, Portuguese_new)
all_classes2 %>% head()

# split labeled, merged dataset into training and testing
set.seed(3435)
grades_split <- initial_split(all_classes2, prop = 0.7,
                              strata = g3)
grades_train <- training(grades_split)
grades_test <- testing(grades_split)
```

#### Method 2. Using one dataset for training, using one dataset for testing

Next, for method two, let's take that large merged dataset we made in the previous step. To use Portuguese for training and Maths for testing, I'm going to group the data by the course variable when we split. With a little bit of math, we can calculate exactly where to split the data so the training is Portuguese data and testing is Maths data. We can check that we split the data correctly by verifying the number of observations in grades_train2 and grades_test2 matches the number of observations in Portuguese and Maths. Which it does, yay! Next, I'm going to remove the course labels from the training and testing datasets, because this may cause problems later on when we try to compare predictions during training/testing.

```{r}
# split labeled dataset by "course" predictor
# Portuguese has 649 observations, Math has 395 observations -> Portuguese/(Portuguese and Math) = 649/1044 = 0.62
set.seed(3435)

grades_split2 <- group_initial_split(all_classes2, group = course,
                                     prop = 0.6234)

# use Portuguese for training
grades_train2 <- training(grades_split2)
# use Maths for testing
grades_test2 <- testing(grades_split2)

# verify number of observations
dim(grades_train2) # all Portuguese observations
dim(grades_test2) # all Maths observations

# remove course labels (may cause problems when we train/test)
grades_train2_nocourse <- grades_train2 %>%
  select(-course)
grades_test2_nocourse <- grades_test2 %>%
  select(-course)
```

## Should we choose Regression or Classification?

The next tricky question we have is to use regression or classification methods. Technically G3 grades are discrete, for example a student can get a grade of 0, 1, 4, 5, but nothing like 1.5 or 4.33. However, from our histogram earlier, we saw that some grades are more commonly earned than others. Specifically, many students earn grades of 0, 9 or 10, but there are very few/no students who earn a grade of 4. Because the grades are so disproportionately spread out, this might cause a problem if we use a classification model. There wouldn't be "enough" grades in each level/category for the model to be trained well. Because of this problem, let's try fitting models in both regression and classification.

Please note: for each of the models below, I will be fitting one model for each method of data splitting. (ex. linear regression model for merged dataset, linear regression model for Portuguese=training, Maths=testing) We will compare the two and see which method of data splitting gives us better predictions.

### Regression

We will make 2 recipes, the first one we use when working with Method 1. merged dataset (has course variable) and the second we use when working with Method 2. Portuguese for training, Maths for testing (without course variable). 

The reason for this is because for Method 2, we split the data into training/testing by course. However, when we fit models and predict, having course=Portuguese for training and course=Math throws an error because the model won't know how to carry over its training knowledge to testing when the two courses are not the same. Therefore we need this 2nd recipe without the course variable, and we need grades_train2_nocourse and grades_test2_nocourse to fit the model.

```{r}
# recipe for Method 1. merged dataset
grades_recipe1 <- recipe(g3 ~ ., data=all_classes2) %>%
  step_dummy(all_nominal_predictors())

# recipe for Method 2. Portuguese for training, Maths for testing
grades_recipe2 <- recipe(g3 ~ ., data=all_classes) %>%
  step_dummy(all_nominal_predictors())
```

We'll begin with a linear regression model.

#### Linear Regression Model (Method 1. merged dataset)

```{r}
# test out a linear regression model on our merged dataset
lm_model <- linear_reg() %>% 
  set_engine("lm")

# create workflow
lm_wflow <- workflow() %>% 
  add_model(lm_model) %>% 
  add_recipe(grades_recipe1)

# fit
lm_fit <- fit(lm_wflow, grades_train)

lm_fit %>% 
  # This returns the parsnip object:
  extract_fit_parsnip() %>% 
  # Now tidy the linear model object:
  tidy()

# put predictions and actual observations together
grades_train_res <- predict(lm_fit, new_data = grades_train %>% select(-g3))
grades_train_res <- bind_cols(grades_train_res, grades_train %>% select(g3))
grades_train_res %>% 
  head()

# plot to visualize
grades_train_res %>% 
  ggplot(aes(x = .pred, y = g3)) +
  geom_point(alpha = 0.2) +
  geom_abline(lty = 2) + 
  theme_bw() +
  coord_obs_pred()

# our metrics
grades_metrics1 <- metric_set(rmse, rsq, mae)
grades_metrics1(grades_train_res, truth = g3, 
                estimate = .pred)
```

#### Linear Regression Model (Method 2. Portuguese for training, Maths for testing)

```{r}
# test out a linear regression model on our second dataset (using Portuguese for training, Maths for testing)
lm_model <- linear_reg() %>% 
  set_engine("lm")

# create workflow
lm_wflow2 <- workflow() %>% 
  add_model(lm_model) %>% 
  add_recipe(grades_recipe2)

# fit
lm_fit <- fit(lm_wflow2, grades_train2_nocourse)

lm_fit %>% 
  # This returns the parsnip object:
  extract_fit_parsnip() %>% 
  # Now tidy the linear model object:
  tidy()

# put predictions and actual observations together
grades_train_res2 <- predict(lm_fit, new_data = grades_train2 %>% select(-g3))
grades_train_res2 <- bind_cols(grades_train_res2, grades_train2_nocourse %>% select(g3))
grades_train_res2 %>% 
  head()

# plot to visualize
grades_train_res2 %>% 
  ggplot(aes(x = .pred, y = g3)) +
  geom_point(alpha = 0.2) +
  geom_abline(lty = 2) + 
  theme_bw() +
  coord_obs_pred()

# our metrics
grades_metrics2 <- metric_set(rmse, rsq, mae)
grades_metrics2(grades_train_res2, truth = g3, 
                estimate = .pred)
```

From our two linear regression models above, we see that both have values of rmse of greater than 1. This shows that the model didn't perform particularly well, or isn't optimized well. For reference, the closer rmse is to 0, the better our model is. We also have a high mae, which isn't super great because the closer mae is to 0, the better. Between the two, it looks like Method 2. Portuguese for training and Maths for testing gave us better predictions. Let's try some classification methods now, and see if they are better at predicting.


### Classification

Before we try fitting classification methods, we should remember the issue discussed earlier. For classification methods, we will need to convert G3 grades to a factor type with different levels. However, because some grades were "more common" (earned by more students than others), there may not be enough student grades data in each "grade category". (ex. grade 0, grade 1, grade 2, etc.)
If we create a level for each individual grade, we will run into the problem of disproportionate categories.
Instead, (advised by my TA Hanmo) we can group the G3 grades into two levels of "Pass" and "Fail." The split between "Pass" and "Fail" is arbitrarily decided, it can be changed depending on the context/use of the model. I simply chose to group grades 0-10 as "Fail", and grades 11-20 as "Pass." Creating two new factor levels will hopefully remedy our problem of not having enough student grades in each category/level, because we are grouping much more data together.

Before we fit any models however, we need to convert our outcome variable G3 into a factor and give it the new levels of "Pass" and "Fail." Again, we do this for the dataset that has course labels, and the dataset without course labels. Be sure to check that the new factor levels have been assigned correctly!

```{r}
# convert outcome variable to factor first!

# Method 1. dataset without course labels
all_classes_fctr <- all_classes %>% mutate(g3 = factor(g3))
all_classes_fctr$g3 %>% head()

# if G3 between 0 and 10, level = Fail, if G3 between 11 and 20, level = Pass
levels(all_classes_fctr$g3) <- list("Fail"="0", "Fail"="1", "Fail"="4", "Fail"="5", "Fail"="6", "Fail"="7", "Fail"="8", "Fail"="9", "Fail"="10", "Pass"="11", "Pass"="12", "Pass"="13", "Pass"="14", "Pass"="15", "Pass"="16", "Pass"="17", "Pass"="18", "Pass"="19", "Pass"="20")

# check that levels were reassigned correctly
all_classes_fctr$g3 %>% head()


# Method 2. dataset with course labels
all_classes2_fctr <- all_classes2 %>% mutate(g3 = factor(g3))
all_classes2_fctr$g3 %>% head()

# if G3 between 0 and 10, level = Fail, if G3 between 11 and 20, level = Pass
levels(all_classes2_fctr$g3) <- list("Fail"="0", "Fail"="1", "Fail"="4", "Fail"="5", "Fail"="6", "Fail"="7", "Fail"="8", "Fail"="9", "Fail"="10", "Pass"="11", "Pass"="12", "Pass"="13", "Pass"="14", "Pass"="15", "Pass"="16", "Pass"="17", "Pass"="18", "Pass"="19", "Pass"="20")

# check that levels were reassigned correctly
all_classes2_fctr$g3 %>% head()
```

We take our datasets with the new factor outcome variable and split it with two different ways, just like we did above. (1. large merged dataset, 2. Portuguese=training, Maths=testing)

```{r}
# Method 1. merged dataset
# split data into training/testing
grades_split_fctr <- initial_split(all_classes2_fctr, prop = 0.7,
                               strata = g3)
grades_train_fctr <- training(grades_split_fctr)
grades_test_fctr <- testing(grades_split_fctr)


# Method 2. Using one dataset for training, using one dataset for testing
# split data into training/testing
grades_split2_fctr <- group_initial_split(all_classes2_fctr, group = course,
                                     prop = 0.6234)
# use Portuguese for training
grades_train2_fctr <- training(grades_split2_fctr)
# use Maths for testing
grades_test2_fctr <- testing(grades_split2_fctr)

# remove course labels (may cause problems when we train/test)
grades_train2_fctr_nocourse <- grades_train2_fctr %>%
  select(-course)
grades_test2_fctr_nocourse <- grades_test2_fctr %>%
  select(-course)
```

Let's make 2 recipes (like we did above) for the classification models with the data, except this time our outcome G3 is a factor.

```{r}
# recipe for Method 1. merged dataset
grades_recipe1_fctr <- recipe(g3 ~ ., data=all_classes2_fctr) %>%
  step_dummy(all_nominal_predictors())

# recipe for Method 2. Portuguese for training, Maths for testing
grades_recipe2_fctr <- recipe(g3 ~ ., data=all_classes_fctr) %>%
  step_dummy(all_nominal_predictors())
```

For the classification models, let's begin with logistic regression. Then we'll try linear discriminant analysis (LDA) and quadratic discriminant analysis (QDA).

#### Logistic Regression Model (Method 1. merged dataset)

```{r}
# test out a logistic regression model on our merged dataset
log_reg <- logistic_reg() %>% 
  set_engine("glm")

# create workflow
log_wkflow <- workflow() %>% 
  add_model(log_reg) %>% 
  add_recipe(grades_recipe1_fctr)

# fit
log_fit <- fit(log_wkflow, grades_train_fctr)
log_fit %>% 
  tidy()

# predictions
predict(log_fit, new_data = grades_train_fctr, type = "prob")

# confusion matrix
augment(log_fit, new_data = grades_train_fctr) %>%
  conf_mat(truth = g3, estimate = .pred_class)

# accuracy
log_reg_acc <- augment(log_fit, new_data = grades_train_fctr) %>%
  accuracy(truth = g3, estimate = .pred_class)
log_reg_acc
```

#### Logistic Regression Model (Method 2. Portuguese for training, Maths for testing)

```{r}
# test out a logistic regression model on our second dataset (using Portuguese for training, Maths for testing)
log_reg <- logistic_reg() %>% 
  set_engine("glm")

# create workflow
log_wkflow2 <- workflow() %>% 
  add_model(log_reg) %>% 
  add_recipe(grades_recipe2_fctr)

# fit
log_fit2 <- fit(log_wkflow2, grades_train2_fctr)
log_fit2 %>% 
  tidy()

# predict
predict(log_fit2, new_data = grades_train2_fctr, type = "prob")

# confusion matrix
augment(log_fit2, new_data = grades_train2_fctr) %>%
  conf_mat(truth = g3, estimate = .pred_class)

# accuracy
log_reg_acc2 <- augment(log_fit2, new_data = grades_train2_fctr) %>%
  accuracy(truth = g3, estimate = .pred_class)
log_reg_acc2
```

#### Linear Discriminant Analysis (LDA) Model (Method 1. merged dataset)

```{r}
# test out a lda model on our merged dataset
lda_mod <- discrim_linear() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")

# create workflow
lda_wkflow <- workflow() %>% 
  add_model(lda_mod) %>% 
  add_recipe(grades_recipe1_fctr)

# fit
lda_fit <- fit(lda_wkflow, grades_train_fctr)

# predict
predict(lda_fit, new_data = grades_train_fctr, type = "prob")

# confusion matrix
augment(lda_fit, new_data = grades_train_fctr) %>%
  conf_mat(truth = g3, estimate = .pred_class)

# accuracy
lda_acc <- augment(lda_fit, new_data = grades_train_fctr) %>%
  accuracy(truth = g3, estimate = .pred_class)
lda_acc
```

#### Linear Discriminant Analysis (LDA) Model (Method 2. Portuguese for training, Maths for testing)

```{r}
# test out a lda model on our second dataset (using Portuguese for training, Maths for testing)
lda_mod <- discrim_linear() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")

# create workflow and add recipe
lda_wkflow2 <- workflow() %>% 
  add_model(lda_mod) %>% 
  add_recipe(grades_recipe2_fctr)

# fit
lda_fit2 <- fit(lda_wkflow2, grades_train2_fctr)

# predict
predict(lda_fit2, new_data = grades_train2_fctr, type = "prob")

# confusion matrix
augment(lda_fit2, new_data = grades_train2_fctr) %>%
  conf_mat(truth = g3, estimate = .pred_class)

# accuracy
lda_acc2 <- augment(lda_fit2, new_data = grades_train2_fctr) %>%
  accuracy(truth = g3, estimate = .pred_class)
lda_acc2
```

#### Quadratic Discriminant Analysis (QDA) Model (Method 1. merged dataset)

```{r}
# test out a qda model on our merged dataset
qda_mod <- discrim_quad() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")

# create workflow
qda_wkflow <- workflow() %>% 
  add_model(qda_mod) %>% 
  add_recipe(grades_recipe1_fctr) # recipe from before

# fit
qda_fit <- fit(qda_wkflow, grades_train_fctr)

# predict
predict(qda_fit, new_data = grades_train_fctr, type = "prob")

# confusion matrix
augment(qda_fit, new_data = grades_train_fctr) %>%
  conf_mat(truth = g3, estimate = .pred_class)

# accuracy
qda_acc <- augment(qda_fit, new_data = grades_train_fctr) %>%
  accuracy(truth = g3, estimate = .pred_class)
qda_acc
```

#### Quadratic Discriminant Analysis (QDA) Model (Method 2. Portuguese for training, Maths for testing)

```{r}
# test out a ada model on our second dataset (using Portuguese for training, Maths for testing)
qda_mod <- discrim_quad() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")

# create workflow
qda_wkflow2 <- workflow() %>% 
  add_model(qda_mod) %>% 
  add_recipe(grades_recipe2_fctr) # recipe from before

# fit
qda_fit2 <- fit(qda_wkflow2, grades_train2_fctr)

# predict
predict(qda_fit2, new_data = grades_train2_fctr, type = "prob")

# confusion matrix
augment(qda_fit2, new_data = grades_train2_fctr) %>%
  conf_mat(truth = g3, estimate = .pred_class)

# accuracy
qda_acc2 <- augment(qda_fit2, new_data = grades_train2_fctr) %>%
  accuracy(truth = g3, estimate = .pred_class)
qda_acc2
```

From the three kinds of models, it seems like logistic regression performed the best, giving us the highest accuracy of 0.9322034. We also notice that for logistic regression and QDA, Method 2 (Portuguese=training, Maths=testing) of data splitting gave higher accuracy. But for QDA, Method 1 (merged dataset) of data splitting gave a higher accuracy.


### k-fold cross validation

We can try k-fold cross validation on the model that performed best out of classification models (logistic regression). Let's use 10 folds and try it on both the methods of data splitting. Will the Method 1. merged dataset or Method 2. Portuguese/Maths split dataset give us better results?

#### Method 1. merged dataset

```{r, message = F, warning = F}
# 10-fold cross validation
grades_folds_fctr <- vfold_cv(grades_train_fctr, v = 10)

# logistic regression model
log_reg <- logistic_reg() %>% 
  set_engine("glm")

# create workflow
log_wkflow <- workflow() %>% 
  add_model(log_reg) %>% 
  add_recipe(grades_recipe1_fctr)

# fit to the folds
fit_log_kfold <- fit_resamples(log_wkflow,
                        grades_folds_fctr,
                        control = control_resamples(save_pred = TRUE)
                        )
# accuracy on training dataset
collect_metrics(fit_log_kfold)

# fit model to entire training dataset
final_fit <- fit(log_wkflow, grades_train_fctr)

# fit model to testing dataset, bind predictions to actual observations
grades_predicted <- predict(final_fit, new_data = grades_test_fctr)
grades_predicted <- bind_cols(grades_predicted, grades_test_fctr$g3)
grades_predicted

# confusion matrix
augment(final_fit, new_data = grades_test_fctr) %>%
  conf_mat(truth = g3, estimate = .pred_class)

# accuracy on testing dataset
multi_metric <- metric_set(accuracy, sensitivity, specificity)
log_acc_kfold <- augment(final_fit, new_data = grades_test_fctr) %>%
  multi_metric(truth = grades_test_fctr$g3, estimate = .pred_class)
log_acc_kfold
```

#### Method 2. Portuguese for training, Maths for testing

```{r, message = F, warning = F}
# 10-fold cross validation
grades_folds2_fctr <- vfold_cv(grades_train2_fctr, v = 10)

# logistic regression model
log_reg <- logistic_reg() %>% 
  set_engine("glm")

# create workflow
log_wkflow2 <- workflow() %>% 
  add_model(log_reg) %>% 
  add_recipe(grades_recipe2_fctr)

# fit model to folds
fit_log2_kfold <- fit_resamples(log_wkflow2,
                        grades_folds2_fctr,
                        control = control_resamples(save_pred = TRUE)
                        )
# accuracy on training dataset
collect_metrics(fit_log2_kfold)

# fit model to entire training dataset
final_fit2 <- fit(log_wkflow2, grades_train2_fctr)

# fit model to testing dataset, bind predictions to actual observations
grades_predicted2 <- predict(final_fit2, new_data = grades_test2_fctr, type = "prob")
grades_predicted2 <- bind_cols(grades_predicted2, grades_test2_fctr$g3)
grades_predicted2

# confusion matrix
augment(final_fit2, new_data = grades_test2_fctr) %>%
  conf_mat(truth = g3, estimate = .pred_class)

# accuracy
multi_metric <- metric_set(accuracy, sensitivity, specificity)
log_acc2_kfold <- augment(final_fit2, new_data = grades_test2_fctr) %>%
  multi_metric(truth = grades_test2_fctr$g3, estimate = .pred_class)
log_acc2_kfold
```

By comparing the testing accuracy, it looks like the model that used Method 1. merged dataset actually gave higher accuracy than Method 2. Portuguese for training, Maths for testing. The accuracy from Method 1. is 0.9235669, the accuracy from Method 2. is 0.8936709. The difference between the two accuracies is only about 0.03. This is opposite of what we saw when we used the training data. Let's plot their respective ROC curves to see if the visual results are consistent.

```{r, message = F, warning = F}
# ROC curves
# Method 1. merged dataset
augment(final_fit, new_data = grades_test_fctr) %>%
  roc_curve(truth = grades_test_fctr$g3, estimate = .pred_Fail) %>%
  autoplot()

# calculate AUC
pred_Fail <- as.data.frame(predict(final_fit, new_data = grades_test_fctr, type = "prob"))[,1]
auc(grades_test_fctr$g3, pred_Fail)

# Method 2. Portuguese for training, Maths for testing
augment(final_fit2, new_data = grades_test2_fctr) %>%
  roc_curve(truth = grades_test2_fctr$g3, estimate = .pred_Fail) %>%
  autoplot()

# calculate AUC
pred_Fail2 <- as.data.frame(predict(final_fit2, new_data = grades_test2_fctr, type = "prob"))[,1]
auc(grades_test2_fctr$g3, pred_Fail2)
```

As expected, the ROC curves of the accuracies from the two different methods are extremely similar, with slight variation at the middle of the curve. If we calculate the AUC (area under the curve) for the two methods, we see that the AUC for Method 1 is 0.9766 which is slightly bigger than the AUC for method 2, 0.9712. This is consistent with our results from the accuracy metrics above.

### Elastic Net Tuning

Next, let's try the elastic net method, where we tune the parameters of mixture and penalty. This is an a model that uses an "in between" penalty term in comparison to ridge and lasso regression (where the penalty terms are 0 and as close to infinity as possible, respectively). Note, we use multinomial regression, which is generally used to predict an outcome with several factor levels. Before we fit the models, we first update our recipes from earlier by centering and scaling our predictors.

```{r}
# update recipes by centering and scaling predictors 
grades_recipe1_fctr_new <- recipe(g3 ~ ., data=all_classes2_fctr) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_center(all_predictors()) %>% # center predictors
  step_scale(all_predictors()) # scale predictors

grades_recipe2_fctr_new <- recipe(g3 ~ ., data=all_classes_fctr) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_center(all_predictors()) %>% # center predictors
  step_scale(all_predictors()) # scale predictors
```

Again, we fit the elastic net model to both our method 1. merged dataset and method 2. Portugese=training, Maths=testing data to see which gives us better results.

#### Method 1. merged dataset

```{r}
# 5-fold cross validation
grades_folds_fctr_new <- vfold_cv(grades_train_fctr, v = 5, strata = g3)

# elastic net model
multinom_reg_model <- multinom_reg(mixture = tune(), penalty = tune()) %>% 
  set_mode("classification") %>%
  set_engine("glmnet")

# create workflow
multinom_reg_wf <- workflow() %>%
  add_model(multinom_reg_model) %>% # add model
  add_recipe(grades_recipe1_fctr_new) # add recipe

# grid for penalty and mixture
grid_pm_en <- grid_regular(mixture(range = c(0, 1)), penalty(range = c(-5, 5)), levels = c(mixture = 10, penalty = 10))
```

```{r, eval = F}
tune_res_elasticnet <- tune_grid(
  object = multinom_reg_wf, 
  resamples = grades_folds_fctr_new, 
  grid = grid_pm_en
)

# write file
write_rds(tune_res_elasticnet, file = "C:/Users/cupca/OneDrive/Documents/UCSB/Fall2022/PSTAT131/Final-Project/Final-Project-laurenrhuang/saved_models/tune_res_elasticnet.rds")
```

```{r}
# read file
tune_res_elasticnet <- read_rds(file = "C:/Users/cupca/OneDrive/Documents/UCSB/Fall2022/PSTAT131//Final-Project/Final-Project-laurenrhuang/saved_models/tune_res_elasticnet.rds")

# autoplot
autoplot(tune_res_elasticnet)
```

```{r}
# show the best models according to roc_auc
show_best(tune_res_elasticnet, metric = "roc_auc")

# select the best
en_best_rocauc <- select_best(tune_res_elasticnet, metric = "roc_auc")
en_best_rocauc

# fit model to training set
elasticnet1_final <- finalize_workflow(multinom_reg_wf, en_best_rocauc)
elasticnet1_final_fit <- fit(elasticnet1_final, data = grades_train_fctr)

# evaluate accuracy
augment(elasticnet1_final_fit, new_data = grades_test_fctr) %>%
  accuracy(truth = g3, estimate = .pred_class)
```

We do the same for Method 2. Portuguese = training, Maths = testing

#### Method 2. Portuguese for training, Maths for testing

```{r}
# 5-fold cross validation
grades_folds2_fctr_new <- vfold_cv(grades_train2_fctr_nocourse, v = 5, strata = g3)

# elastic net model
multinom_reg_model <- multinom_reg(mixture = tune(), penalty = tune()) %>% 
  set_mode("classification") %>%
  set_engine("glmnet")

# create workflow
multinom_reg_wf2 <- workflow() %>%
  add_model(multinom_reg_model) %>% # add model
  add_recipe(grades_recipe2_fctr_new) # add recipe
```

```{r, eval = F}
tune_res_elasticnet2 <- tune_grid(
  object = multinom_reg_wf2, 
  resamples = grades_folds2_fctr_new, 
  grid = grid_pm_en
)

# write file
write_rds(tune_res_elasticnet2, file = "C:/Users/cupca/OneDrive/Documents/UCSB/Fall2022/PSTAT131/Final-Project/Final-Project-laurenrhuang/saved_models/tune_res_elasticnet2.rds")
```

```{r}
# read file
tune_res_elasticnet2 <- read_rds(file = "C:/Users/cupca/OneDrive/Documents/UCSB/Fall2022/PSTAT131/Final-Project/Final-Project-laurenrhuang/saved_models/tune_res_elasticnet2.rds")

# autoplot
autoplot(tune_res_elasticnet2)
```

```{r}
# show the best models according to roc_auc
show_best(tune_res_elasticnet2, metric = "roc_auc")

# select the best
en_best_rocauc2 <- select_best(tune_res_elasticnet2, metric = "roc_auc")
en_best_rocauc2

# fit model to training set
elasticnet2_final <- finalize_workflow(multinom_reg_wf2, en_best_rocauc2)
elasticnet2_final_fit <- fit(elasticnet2_final, data = grades_train2_fctr_nocourse)

# evaluate accuracy
augment(elasticnet2_final_fit, new_data = grades_test2_fctr_nocourse) %>%
  accuracy(truth = g3, estimate = .pred_class)
```

From the results above, it seems like Method 1. merged dataset gives us better accuracy when fitted to the elastic net model. Let's investigate the details of this model by calculating AUC, roc_auc, and plotting ROC curves. We can also make a heatmap of the confusion matrix.

```{r, warning = F, message = F}
# convert predicted values
pred_class <- as.data.frame(predict(elasticnet1_final_fit, new_data = grades_test_fctr, type = "prob"))[,1]

# calculate AUC
auc(grades_test_fctr$g3, pred_class)

# metrics
multi_metric <- metric_set(accuracy, sensitivity, specificity)
augment(elasticnet1_final_fit, new_data = grades_test_fctr) %>%
  multi_metric(truth = g3, estimate = .pred_class)

# ROC curves
augment(elasticnet1_final_fit, new_data = grades_test_fctr) %>%
  roc_curve(truth = g3, estimate = .pred_Fail) %>%
  autoplot()

# confusion matrix
augment(elasticnet1_final_fit, new_data = grades_test_fctr) %>%
  conf_mat(truth = g3, estimate = .pred_class) %>%
  autoplot(type = "heatmap")

# ROC_AUC
predicted_data <- augment(elasticnet1_final_fit, new_data = grades_test_fctr) %>%
  select(g3, starts_with(".pred"))
predicted_data %>% roc_auc(g3, .pred_Fail)
```

Our results match up, yay! our ROC AUC is 0.9799, which follows the ROC curve we plotted. This can also be verified with the results from the confusion matrix. Lastly, our metrics tell us that the elastic net model accuracy on the testing dataset is 0.9299363.

### Decision Tree

Next, we move to the tree models. A decision tree model is just one type of tree model. You can think of a decision tree as a branching method that looks at every possible output for a specific input. Take a look at the visualization in the next code chunk.

#### Method 1. merged dataset

```{r, warning = F}
# decision tree engine
tree_spec <- decision_tree() %>%
  set_engine("rpart")
class_tree_spec <- tree_spec %>%
  set_mode("classification")

# fit model
class_tree_fit <- class_tree_spec %>%
  fit(g3 ~ ., data = grades_train_fctr)

# visualize
class_tree_fit %>%
  extract_fit_engine() %>%
  rpart.plot()
```
Like the previous models, we set an engine for the decision tree model. We also make a grid to tune the cost complexity parameter. Note, the cost complexity parameter is very important because it helps "prune" the tree. This means it reduces the size of the tree and keeps the tree from having "too many branches" or "leaf nodes." A tree that has too many branches or leaf nodes could lead to model overfitting.

```{r}
# decision tree engine
tree_spec <- decision_tree() %>%
  set_engine("rpart") %>%
  set_mode("classification")

# create workflow
class_tree_wf <- workflow() %>%
  add_model(tree_spec %>% set_args(cost_complexity = tune())) %>%
  add_recipe(grades_recipe1_fctr_new)

# create grid
set.seed(3435)
param_grid <- grid_regular(cost_complexity(range = c(-3, -1)), levels = 10)
```

We fit the model with tune_grid, but it can take a long time to run, so it would be wise to save our fitted model to a file. This way, we don't have to refit the model every time we run the code.

```{r, eval = F}
# fit with tune_grid
classtree_tune_res <- tune_grid(
  class_tree_wf, 
  resamples = grades_folds_fctr_new, 
  grid = param_grid, 
  metrics = metric_set(accuracy)
)

# write file
write_rds(tune_res, file = "C:/Users/cupca/OneDrive/Documents/UCSB/Fall2022/PSTAT131/Final-Project/Final-Project-laurenrhuang/saved_models/classtree_tune_res.rds")
```

Read the file, and plot a comparison of the cost-complexity parameter to accuracy. What do you notice? It looks like accuracy peaks and flattens at about 0.92 once we reach a cost complexity of about 0.02.

```{r}
# read file
classtree_tune_res <- read_rds(file = "C:/Users/cupca/OneDrive/Documents/UCSB/Fall2022/PSTAT131/Final-Project/Final-Project-laurenrhuang/saved_models/classtree_tune_res.rds")

# cost-complexity accuracy autoplot
autoplot(classtree_tune_res)
```

```{r}
# show metrics
collect_metrics(classtree_tune_res) %>% arrange(desc(mean))

# select the best
classtree_best_complexity <- select_best(classtree_tune_res)
classtree_best_complexity

# fit
classtree_final1 <- finalize_workflow(class_tree_wf, classtree_best_complexity)
classtree_final1_fit <- fit(classtree_final1, data = grades_train_fctr)

# roc_auc
classtree_predicted_data <- augment(classtree_final1_fit, new_data = grades_train_fctr) %>%
  select(g3, starts_with(".pred"))
classtree_predicted_data %>% roc_auc(g3, .pred_Fail)
```

Showing the metrics will give us the specific numbers of cost complexity parameters. From this, we can choose the one that gives us the best accuracy. In this case, the best accuracy is 0.9014038. Using the corresponding best cost complexity parameter gives us an ROC AUC of 0.9048427.


#### Method 2. Portuguese for training, Maths for testing

We can do the same for Method 2 of data splitting.

```{r, warning = F}
# decision tree engine
tree_spec <- decision_tree() %>%
  set_engine("rpart")
class_tree_spec <- tree_spec %>%
  set_mode("classification")

# fit model
class_tree_fit2 <- class_tree_spec %>%
  fit(g3 ~ ., data = grades_train2_fctr)

# visualize
class_tree_fit %>%
  extract_fit_engine() %>%
  rpart.plot()
```


```{r}
# decision tree engine
tree_spec <- decision_tree() %>%
  set_engine("rpart") %>%
  set_mode("classification")

# create workflow
class_tree_wf2 <- workflow() %>%
  add_model(tree_spec %>% set_args(cost_complexity = tune())) %>%
  add_recipe(grades_recipe2_fctr_new)
```

```{r, eval = F}
# fit with tune_grid
classtree_tune_res2 <- tune_grid(
  class_tree_wf2, 
  resamples = grades_folds2_fctr_new, 
  grid = param_grid, 
  metrics = metric_set(accuracy)
)
# write file
write_rds(classtree_tune_res2, file = "C:/Users/cupca/OneDrive/Documents/UCSB/Fall2022/PSTAT131/Final-Project/Final-Project-laurenrhuang/saved_models/classtree_tune_res2.rds")
```

```{r}
# read file
classtree_tune_res2 <- read_rds(file = "C:/Users/cupca/OneDrive/Documents/UCSB/Fall2022/PSTAT131/Final-Project/Final-Project-laurenrhuang/saved_models/classtree_tune_res2.rds")
# cost-complexity accuracy autoplot
autoplot(classtree_tune_res2)
```

```{r}
# show metrics
collect_metrics(classtree_tune_res2) %>% arrange(desc(mean))

# select the best
classtree_best_complexity2 <- select_best(classtree_tune_res2)
classtree_best_complexity2

# fit
classtree_final2 <- finalize_workflow(class_tree_wf2, classtree_best_complexity2)
classtree_final_fit2 <- fit(classtree_final2, data = grades_train2_fctr_nocourse)

# roc_auc
classtree_predicted_data2 <- augment(classtree_final_fit2, new_data = grades_train2_fctr_nocourse) %>%
  select(g3, starts_with(".pred"))
classtree_predicted_data2 %>% roc_auc(g3, .pred_Fail)
```

Showing the metrics will give us the specific numbers of cost complexity parameters. We again can choose the one that gives us the best accuracy. Here, the best accuracy is 0.9029647. Using the corresponding best cost complexity parameter gives us an ROC AUC of 0.9338024.

### Boosted Tree

Next, let's try a boosted tree model to see if it gives us better results than the decision tree model. A boosted tree is like a decision tree, but boosting means that each tree is dependent on prior trees. The algorithm learns by fitting the residual of the trees that preceded it. Boosting uses a group of decision tree models.

#### Method 1. merged dataset

First, specify the boosted tree engine and create a grid to tune the number of trees.

```{r}
# set boosted tree engine
xgb_spec <- boost_tree() %>%
  set_engine("xgboost") %>%
  set_mode("classification")

# create workflow
xgb_wf <- workflow() %>%
  add_model(xgb_spec %>% set_args(trees = tune())) %>%
  add_recipe(grades_recipe1_fctr_new)

# create grid
param_grid2 <- grid_regular(trees(range = c(10, 2000)),
                            levels = 10)
```

Fit using tune_grid, and write to file.

```{r, eval = F}
# fit using tune_grid
boostedtree_tune_res <- tune_grid(
  xgb_wf, 
  resamples = grades_folds_fctr_new, 
  grid = param_grid2, 
  metrics = metric_set(accuracy)
)
# write file
write_rds(boostedtree_tune_res, file = "C:/Users/cupca/OneDrive/Documents/UCSB/Fall2022/PSTAT131/Final-Project/Final-Project-laurenrhuang/saved_models/boostedtree_tune_res.rds")
```

Let's read the file and plot the accuracies. It looks like accuracy peaks at 0.8972186 around 1115 trees. 

```{r}
# read file
boostedtree_tune_res <- read_rds(file = "C:/Users/cupca/OneDrive/Documents/UCSB/Fall2022/PSTAT131/Final-Project/Final-Project-laurenrhuang/saved_models/boostedtree_tune_res.rds")

# autoplot
autoplot(boostedtree_tune_res)

# show metrics
collect_metrics(boostedtree_tune_res) %>% arrange(desc(mean))

# select the best
boostedtree_best_complexity <- select_best(boostedtree_tune_res)
boostedtree_best_complexity

# fit
xgb_final1 <- finalize_workflow(xgb_wf, boostedtree_best_complexity)
xgb_final_fit1 <- fit(xgb_final1, data = grades_train_fctr)

# roc_auc
xgb_predicted_data <- augment(xgb_final_fit1, new_data = grades_train_fctr) %>%
  select(g3, starts_with(".pred"))
xgb_predicted_data %>% roc_auc(g3, .pred_Fail)
```

The highest accuracy we get is 0.8972186, but we get an ROC AUC of 1. Why do you think this is?

#### Method 2. Portuguese for training, Maths for testing

Here we'll do the same, except using Method 2 of data splitting. 

```{r}
# set boosted tree engine
xgb_spec <- boost_tree() %>%
  set_engine("xgboost") %>%
  set_mode("classification")

# create workflow
xgb_wf2 <- workflow() %>%
  add_model(xgb_spec %>% set_args(trees = tune())) %>%
  add_recipe(grades_recipe2_fctr_new)

# create grid
param_grid2 <- grid_regular(trees(range = c(10, 2000)),
                            levels = 10)
```

```{r, eval = F}
# fit using tune_grid
boostedtree_tune_res2 <- tune_grid(
  xgb_wf2, 
  resamples = grades_folds2_fctr_new, 
  grid = param_grid2, 
  metrics = metric_set(accuracy)
)
# write file
write_rds(boostedtree_tune_res2, file = "C:/Users/cupca/OneDrive/Documents/UCSB/Fall2022/PSTAT131/Final-Project/Final-Project-laurenrhuang/saved_models/boostedtree_tune_res2.rds")
```

Read the file and plot the accuracies. This time it looks like accuracy peaks at 0.8981952 with about 231 trees.

```{r}
# read file
boostedtree_tune_res2 <- read_rds(file = "C:/Users/cupca/OneDrive/Documents/UCSB/Fall2022/PSTAT131/Final-Project/Final-Project-laurenrhuang/saved_models/boostedtree_tune_res2.rds")

# autoplot
autoplot(boostedtree_tune_res2)

# show metrics
collect_metrics(boostedtree_tune_res2) %>% arrange(desc(mean))

# select the best
boostedtree_best_complexity2 <- select_best(boostedtree_tune_res2)
boostedtree_best_complexity2

# fit
xgb_final2 <- finalize_workflow(xgb_wf2, boostedtree_best_complexity2)
xgb_final_fit2 <- fit(xgb_final2, data = grades_train2_fctr_nocourse)

# roc_auc
xgb_predicted_data2 <- augment(xgb_final_fit2, new_data = grades_train2_fctr_nocourse) %>%
  select(g3, starts_with(".pred"))
xgb_predicted_data2 %>% roc_auc(g3, .pred_Fail)
```

Again, we have a lower accuracy of 0.8981952 compared to the decision tree models, and the ROC AUC is 1 again. Let's try fitting the boosted tree model to our testing data and see how it performs.

In review, after analyzing both of our boosted tree models, Method 2 of data splitting gave us a slightly higher accuracy of 0.8981952. Let us now fit the testing data to our Method 2 model. We can observe the results by making a ROC curve and heatmap of the confusion matrix to see if the results line up with our ROC AUC value.

```{r}
# select the best (boosted tree)
best_rocauc <- select_best(boostedtree_tune_res2)

# fit
rocauc_final <- finalize_workflow(xgb_wf2, boostedtree_best_complexity2)
rocauc_final_fit <- fit(rocauc_final, data = grades_train2_fctr_nocourse)

# roc_auc
predicted_data_final <- augment(xgb_final_fit2, new_data = grades_test2_fctr_nocourse) %>%
  select(g3, starts_with(".pred"))
predicted_data_final %>% roc_auc(g3, .pred_Fail)

# roc curves
augment(rocauc_final_fit, new_data = grades_test2_fctr_nocourse) %>%
  roc_curve(truth = g3, estimate = .pred_Fail) %>%
  autoplot()

# confusion matrix
augment(rocauc_final_fit, new_data = grades_test2_fctr_nocourse) %>%
  conf_mat(truth = g3, estimate = .pred_class) %>%
  autoplot(type = "heatmap")
```

