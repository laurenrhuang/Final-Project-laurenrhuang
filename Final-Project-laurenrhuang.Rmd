---
title: "Final Project"
author: "Lauren Huang"
date: "2022-10-14"
output:
  html_document:
    toc: true
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Predicting students' grades in two Portuguese schools

A brief introduction to the selected data set: The data contains information regarding students' academic achievement in secondary education of two Portuguese schools. There are two datasets. One for Maths class, one for Portuguese class. Both datasets contain the same predictor variables, including but not limited to: school, sex, age, address, family size, parent status, mother's education, father's education, mother's job, father's job, etc. There are also variables called G1, G2 and G3. G1 represents a student's grade during first period, G2 represents a student's grade during second period. G3 represents a student's final grade in the class. Having both G1 and G2 are extremely helpful in predicting G3, which is a student's final grade in a class.

## Reading in and cleaning the data

The first step is to install packages for the functions or models we want to fit. For the framework of our models, we'll be using tidymodels.

```{r, message = F, warning = F}
# load packages
library(tidyverse)
library(tidymodels)
library(MASS)
library(discrim)
library(poissonreg)
library(corrr)
library(corrplot)
library(pROC)
library(janitor)
library(rpart.plot)
library(vip)
library(randomForest)
library(xgboost)
tidymodels_prefer()
```

After installing necessary packages, we read in our two datasets for the classes "Maths" and "Portuguese."

```{r}
# read in the Maths dataset
Maths <- read_csv("C:/Users/cupca/OneDrive/Documents/UCSB/Fall2022/PSTAT131/Final-Project/Final-Project-laurenrhuang/data/Maths.csv", show_col_types = FALSE)

# clean variable names
Maths <- clean_names(Maths)
Maths %>% head()

# number of observations in Maths
dim(Maths)

# read in the Portuguese dataset
Portuguese <- read_csv("C:/Users/cupca/OneDrive/Documents/UCSB/Fall2022/PSTAT131/Final-Project/Final-Project-laurenrhuang/data/Portuguese.csv", show_col_types = FALSE)

# clean variable names
Portuguese <- clean_names(Portuguese)
Portuguese %>% head()

# number of observations in Portuguese
dim(Portuguese)
```

## Exploratory Data Analysis

Before we do any analysis, we should get to know our data better first. Let's take a look at the variables, the number of observations in each dataset, and make histograms for each of the datasets to see how G3 student grades are distributed.

### Histogram of the Data

```{r}
# distribution of G3 grades in Maths
hist(Maths$g3,
     breaks=19,
     xlab = "student final grades (G3) in Maths",
     ylab = "count",
     col = "light blue")

# distribution of G3 grades in Portuguese
hist(Portuguese$g3,
     breaks=19,
     xlab = "student final grades (G3) in Portuguese",
     ylab = "count",
     col = "orange")
```

From the histograms, it looks like most students' G3 grades are concentrated between 5 and 20, with the most common grades being around 10. Maths G3 grades are slightly more centered, while Portuguese G3 grades are slightly right skewed. Interestingly, there are also many students whose Maths G3 grades are 0.

### Correlation Matrix of the Variables

We can also look at make a correlation matrix to see the relationship between each of the variables in the datasets.

```{r}
# handle non-numeric variables
Maths_numeric <- Maths %>% select(-school, -sex, -age, -address, -famsize, -pstatus, -mjob, -fjob, -reason, -guardian, -schoolsup, -famsup, -paid, -activities, -nursery, -higher, -internet, -romantic)

# Maths correlation matrix
Maths_cor = cor(Maths_numeric)
corrplot(Maths_cor)


# handle non-numeric variables
Portuguese_numeric <- Portuguese %>% select(-school, -sex, -age, -address, -famsize, -pstatus, -mjob, -fjob, -reason, -guardian, -schoolsup, -famsup, -paid, -activities, -nursery, -higher, -internet, -romantic)

# Portuguese correlation matrix
Portuguese_cor = cor(Portuguese_numeric)
corrplot(Portuguese_cor)
```

The first thing that stands out to me among the two matrices is that several of the predictors are more strongly correlated (whether negative or positive) in the Portuguese dataset than the Maths dataset. You can tell by the color and size of the dots, some of the dots in the Portuguese correlation matrix tend to be darker in color and larger in size. One example of this is relationship between G1, G2 grades and failures. Are there any other relationships that stand out to you?

As we mentioned earlier, the goal of using this data will be using our predictor variables (ex. school, sex, age, address, family size, parental status, mother's education, father's education, etc.) to predict a student's G3 (final grade) grade/whether a student will pass the class. As we look through the variables, we see that G1 and G2 grades are included. This is great because they will help us in predicting G3 grades.

Let's add a variable called "course" so we know which observations are from which dataset. This is important later when we merge and split the dataset, we will want to keep track.

```{r}
# add course = "Maths" as a predictor to label the dataset
course <- rep("maths", times=395)
Maths_new <- cbind(Maths, course)
Maths_new <- clean_names(Maths_new)
Maths_new %>% head()

# add course = "Portuguese" as a predictor to label the dataset
course <- rep("portuguese", times=649)
Portuguese_new <- cbind(Portuguese, course)
Portuguese_new <- clean_names(Portuguese_new)
Portuguese_new %>% head()
```

### How should we split the 2 datasets?

Since we have 2 datasets and not just 1, there are more options with how we can split and use them. Let's try two different ways: 
  1. Method 1 will be to take both the Maths and Portuguese datasets and merge them into one big dataset.
  2. Method 2 will be to use one dataset to train, and the other to test our models. I will be using Portuguese to train and Maths to test because Portuguese has more observations (the proportion of training data should be bigger than the proportion of testing data!)

We can fit the models to both methods of data splitting to see which gives us more accurate predictions.

#### Method 1. Merging the data

For method 1, let's merge both Maths and Portuguese into one large dataset.  Mainly we will be working with the data that is labeled by course. After merging, split the merged dataset into training dataset and testing dataset, stratifying on G3, our outcome variable. Stratifying on G3 helps us keep the proportions of the original dataset when we split into training and testing! (You'll notice I'm also merging the data that doesn't have the course variable because I'll need to use it later on.)

```{r}
# merge dataset (without labels) into one large dataset 
all_classes <- rbind(Maths, Portuguese) # for train/test use later on in the project
all_classes %>% head()

# merge course labeled Maths and Portuguese datasets into one large dataset
all_classes2 <- rbind(Maths_new, Portuguese_new)
all_classes2 %>% head()

# split labeled, merged dataset into training and testing
set.seed(3435)
grades_split <- initial_split(all_classes2, prop = 0.7,
                              strata = g3)
grades_train <- training(grades_split)
grades_test <- testing(grades_split)
```

#### Method 2. Using one dataset for training, using one dataset for testing

For method 2, let's take the large merged dataset we made in the previous step. To use Portuguese for training and Maths for testing, I'm going to group the data by the course variable when we split. With a little bit of math, we can calculate exactly where to split the data so the training is Portuguese data and testing is Maths data.
We can check that we split the data correctly by verifying the number of observations in grades_train2 and grades_test2 matches the number of observations in Portuguese and Maths. Which it does! Next, I'm going to remove the course labels from the training and testing datasets, because this may cause problems later on when we try to compare predictions with the training/testing data.

```{r}
# split labeled dataset by "course" predictor
set.seed(3435)
# Portuguese has 649 observations, Math has 395 observations -> Portuguese/(Portuguese and Math) = 649/1044 = 0.62
grades_split2 <- group_initial_split(all_classes2, group = course,
                                     prop = 0.6234)

# use Portuguese for training
grades_train2 <- training(grades_split2)
# use Maths for testing
grades_test2 <- testing(grades_split2)

# verify number of observations
dim(grades_train2) # all Portuguese observations
dim(grades_test2) # all Maths observations

# remove course labels (may cause problems when we train/test)
grades_train2_nocourse <- grades_train2 %>%
  select(-course)
grades_test2_nocourse <- grades_test2 %>%
  select(-course)
```

## Should we choose Regression or Classification?

The next tricky question we have is to use regression or classification methods. Technically G3 grades are discrete, for example a student can get a grade of 0, 1, 4, 5, but nothing like 1.5 or 4.33. However, from our histogram earlier, we saw that some grades are more commonly earned than others. Specifically, many students earn grades of 0, 9 or 10, but there are very few/no students who earn a grade of 4. Because the grades are so disproportionately spread out, this might cause a problem if we use a classification model. There wouldn't be "enough" grades in each level/category for the model to be trained well. Because of this problem, let's try fitting models in both regression and classification.

Note: From here onward, I will be fitting two models per method. This represents one model for each method of data splitting. (ex. linear regression model for Method 1.merged dataset, linear regression model for Method 2. Portuguese=training, Maths=testing) We will then compare the models against each method and see which gives us better predictions.

### Regression

We will make 2 recipes, the first one we use when working with Method 1. merged dataset (has course variable) and the second we use when working with Method 2. Portuguese for training, Maths for testing (without course variable). 

The reason for this is because for Method 2, we split the data into training/testing by course. However, when we fit models and predict, having course=Portuguese for training and course=Math for testing throws an error because the model won't know how to carry over its knowledge when there is only one level (Portuguese) in the training data, and one level (Maths) in the testing data. Therefore we need this 2nd recipe without the course variable, and we need grades_train2_nocourse and grades_test2_nocourse to fit the model.

```{r}
# recipe for Method 1. merged dataset
grades_recipe1 <- recipe(g3 ~ ., data=all_classes2) %>%
  step_dummy(all_nominal_predictors())

# recipe for Method 2. Portuguese for training, Maths for testing
grades_recipe2 <- recipe(g3 ~ ., data=all_classes) %>%
  step_dummy(all_nominal_predictors())
```

We'll begin with a linear regression model.

#### Linear Regression Model (Method 1. merged dataset)

```{r}
lm_model <- linear_reg() %>% 
  set_engine("lm")

# create workflow
lm_wflow <- workflow() %>% 
  add_model(lm_model) %>% 
  add_recipe(grades_recipe1)

# fit
lm_fit <- fit(lm_wflow, grades_train)

lm_fit %>% 
  # This returns the parsnip object:
  extract_fit_parsnip() %>% 
  # Now tidy the linear model object:
  tidy()

# put predictions and actual observations together
grades_train_res <- predict(lm_fit, new_data = grades_train %>% select(-g3))
grades_train_res <- bind_cols(grades_train_res, grades_train %>% select(g3))
grades_train_res %>% 
  head()

# plot to visualize
grades_train_res %>% 
  ggplot(aes(x = .pred, y = g3)) +
  geom_point(alpha = 0.2) +
  geom_abline(lty = 2) + 
  theme_bw() +
  coord_obs_pred()

# our metrics
grades_metrics1 <- metric_set(rmse, rsq, mae)
grades_metrics1(grades_train_res, truth = g3, 
                estimate = .pred)
```

#### Linear Regression Model (Method 2. Portuguese for training, Maths for testing)

```{r}
lm_model <- linear_reg() %>% 
  set_engine("lm")

# create workflow
lm_wflow2 <- workflow() %>% 
  add_model(lm_model) %>% 
  add_recipe(grades_recipe2)

# fit
lm_fit <- fit(lm_wflow2, grades_train2_nocourse)

lm_fit %>% 
  # This returns the parsnip object:
  extract_fit_parsnip() %>% 
  # Now tidy the linear model object:
  tidy()

# put predictions and actual observations together
grades_train_res2 <- predict(lm_fit, new_data = grades_train2 %>% select(-g3))
grades_train_res2 <- bind_cols(grades_train_res2, grades_train2_nocourse %>% select(g3))
grades_train_res2 %>% 
  head()

# plot to visualize
grades_train_res2 %>% 
  ggplot(aes(x = .pred, y = g3)) +
  geom_point(alpha = 0.2) +
  geom_abline(lty = 2) + 
  theme_bw() +
  coord_obs_pred()

# our metrics
grades_metrics2 <- metric_set(rmse, rsq, mae)
grades_metrics2(grades_train_res2, truth = g3, 
                estimate = .pred)
```

From our two linear regression models above, we see that both have values of rmse of greater than 1. This shows that the model didn't perform particularly well, or isn't optimized well. We have a high rmse, and generally the closer rmse is to 0, the better our model is. We also have a high mae, which isn't great because the closer mae is to 0, the better. Between the two models, it looks like Method 2. gave us better predictions. Let's try some classification methods instead and see if they perform better!


### Classification

Before we try fitting classification methods, we should remember the issue discussed earlier. For classification methods, we will need to convert G3 grades to a factor type with different levels. However, because some grades were "more common" (earned by more students than others), there may not be enough student grades data in each "grade category". (ex. grade 0, grade 1, grade 2, etc.)
If we create a level for each individual grade, we will run into the problem of disproportionate categories.
Instead, (advised by my TA Hanmo) we can group the G3 grades into two levels of "Pass" and "Fail." The split between "Pass" and "Fail" is arbitrarily decided, it can be changed depending on the context/use of the model. I simply chose to group grades 0-10 as "Fail", and grades 11-20 as "Pass." Creating two new factor levels will hopefully remedy our problem of not having enough student grades in each category/level, because we are grouping much more data together.

Before we fit any models however, we need to convert our outcome variable G3 into a factor and give it the new levels of "Pass" and "Fail." Again, we do this for the dataset that has course labels, and the dataset without course labels. Be sure to check that the new factor levels have been assigned correctly!

```{r}
# convert outcome variable to factor first!

# Method 1. dataset without course labels
all_classes_fctr <- all_classes %>% mutate(g3 = factor(g3))
all_classes_fctr$g3 %>% head()

# if G3 between 0 and 10, level = Fail, if G3 between 11 and 20, level = Pass
levels(all_classes_fctr$g3) <- list("Fail"="0", "Fail"="1", "Fail"="4", "Fail"="5", "Fail"="6", "Fail"="7", "Fail"="8", "Fail"="9", "Fail"="10", "Pass"="11", "Pass"="12", "Pass"="13", "Pass"="14", "Pass"="15", "Pass"="16", "Pass"="17", "Pass"="18", "Pass"="19", "Pass"="20")

# check that levels were reassigned correctly
all_classes_fctr$g3 %>% head()


# Method 2. dataset with course labels
all_classes2_fctr <- all_classes2 %>% mutate(g3 = factor(g3))
all_classes2_fctr$g3 %>% head()

# if G3 between 0 and 10, level = Fail, if G3 between 11 and 20, level = Pass
levels(all_classes2_fctr$g3) <- list("Fail"="0", "Fail"="1", "Fail"="4", "Fail"="5", "Fail"="6", "Fail"="7", "Fail"="8", "Fail"="9", "Fail"="10", "Pass"="11", "Pass"="12", "Pass"="13", "Pass"="14", "Pass"="15", "Pass"="16", "Pass"="17", "Pass"="18", "Pass"="19", "Pass"="20")

# check that levels were reassigned correctly
all_classes2_fctr$g3 %>% head()
```

We take our datasets with the new factor outcome variable and split it with two different ways, just like we did above. Method 1. merged dataset, Method 2. Portuguese = training, Maths = testing

```{r}
set.seed(3435)

# Method 1. split data into training/testing
grades_split_fctr <- initial_split(all_classes2_fctr, prop = 0.7,
                               strata = g3)
grades_train_fctr <- training(grades_split_fctr)
grades_test_fctr <- testing(grades_split_fctr)


# Method 2. split data into training/testing
grades_split2_fctr <- group_initial_split(all_classes2_fctr, group = course,
                                     prop = 0.6234)
# use Portuguese for training
grades_train2_fctr <- training(grades_split2_fctr)
# use Maths for testing
grades_test2_fctr <- testing(grades_split2_fctr)

# remove course labels (may cause problems when we train/test)
grades_train2_fctr_nocourse <- grades_train2_fctr %>%
  select(-course)
grades_test2_fctr_nocourse <- grades_test2_fctr %>%
  select(-course)
```

Let's make 2 recipes (like we did above) for the classification models with the data, except this time our outcome G3 is a factor.

```{r}
# recipe for Method 1. merged dataset
grades_recipe1_fctr <- recipe(g3 ~ ., data=all_classes2_fctr) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_center(all_predictors()) %>% # center predictors
  step_scale(all_predictors()) # scale predictors

# recipe for Method 2. Portuguese for training, Maths for testing
grades_recipe2_fctr <- recipe(g3 ~ ., data=all_classes_fctr) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_center(all_predictors()) %>% # center predictors
  step_scale(all_predictors()) # scale predictors
```

For the classification models, here are the steps we need to take:

1. Set up the model by specifying the model type, engine, and mode.

2. Set up the workflow, add the new model, and add the established recipe.

*Skip steps 3-5 for Log Regression, LDA, and QDA

3. Set up the tuning grid with the parameters and different levels of tuning we want.

4. Tune the model with certain parameters of choice.

5. Select the most accurate model from all of the tuning, finalize the workflow.

6. Fit that model to the training data set.

7. Save the results to file to cut down on runtime when we rerun the code.

8. Repeat steps for Method 2, we need to try both methods to see which one gives more accurate results.


Let's begin with logistic regression. Then we'll try linear discriminant analysis (LDA) and quadratic discriminant analysis (QDA).

#### Logistic Regression Model (Method 1. merged dataset)

```{r}
log_reg <- logistic_reg() %>% 
  set_engine("glm")

# create workflow
log_wkflow <- workflow() %>% 
  add_model(log_reg) %>% 
  add_recipe(grades_recipe1_fctr)

# fit
log_fit <- fit(log_wkflow, grades_train_fctr)
log_fit %>% 
  tidy()

# predictions
predict(log_fit, new_data = grades_train_fctr, type = "prob")

# confusion matrix
augment(log_fit, new_data = grades_train_fctr) %>%
  conf_mat(truth = g3, estimate = .pred_class)

# accuracy
log_reg_acc <- augment(log_fit, new_data = grades_train_fctr) %>%
  accuracy(truth = g3, estimate = .pred_class)
log_reg_acc
```

#### Logistic Regression Model (Method 2. Portuguese for training, Maths for testing)

```{r}
log_reg <- logistic_reg() %>% 
  set_engine("glm")

# create workflow
log_wkflow2 <- workflow() %>% 
  add_model(log_reg) %>% 
  add_recipe(grades_recipe2_fctr)

# fit
log_fit2 <- fit(log_wkflow2, grades_train2_fctr)
log_fit2 %>% 
  tidy()

# predict
predict(log_fit2, new_data = grades_train2_fctr, type = "prob")

# confusion matrix
augment(log_fit2, new_data = grades_train2_fctr) %>%
  conf_mat(truth = g3, estimate = .pred_class)

# accuracy
log_reg_acc2 <- augment(log_fit2, new_data = grades_train2_fctr) %>%
  accuracy(truth = g3, estimate = .pred_class)
log_reg_acc2
```

Both data methods gave us accuracies greater than 0.9, that's really good! Will LDA or QDA give us better results?

#### Linear Discriminant Analysis (LDA) Model (Method 1. merged dataset)

```{r}
lda_mod <- discrim_linear() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")

# create workflow
lda_wkflow <- workflow() %>% 
  add_model(lda_mod) %>% 
  add_recipe(grades_recipe1_fctr)

# fit
lda_fit <- fit(lda_wkflow, grades_train_fctr)

# predict
predict(lda_fit, new_data = grades_train_fctr, type = "prob")

# confusion matrix
augment(lda_fit, new_data = grades_train_fctr) %>%
  conf_mat(truth = g3, estimate = .pred_class)

# accuracy
lda_acc <- augment(lda_fit, new_data = grades_train_fctr) %>%
  accuracy(truth = g3, estimate = .pred_class)
lda_acc
```

#### Linear Discriminant Analysis (LDA) Model (Method 2. Portuguese for training, Maths for testing)

```{r}
lda_mod <- discrim_linear() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")

# create workflow
lda_wkflow2 <- workflow() %>% 
  add_model(lda_mod) %>% 
  add_recipe(grades_recipe2_fctr)

# fit
lda_fit2 <- fit(lda_wkflow2, grades_train2_fctr)

# predict
predict(lda_fit2, new_data = grades_train2_fctr, type = "prob")

# confusion matrix
augment(lda_fit2, new_data = grades_train2_fctr) %>%
  conf_mat(truth = g3, estimate = .pred_class)

# accuracy
lda_acc2 <- augment(lda_fit2, new_data = grades_train2_fctr) %>%
  accuracy(truth = g3, estimate = .pred_class)
lda_acc2
```

The accuracies from LDA are slightly lower than what we got for logistic regression. If we were to pick one of the two models, we should probably go with logistic regression, because it is very similar to LDA, but less complex in some aspects. But before we choose, let's check QDA too.

#### Quadratic Discriminant Analysis (QDA) Model (Method 1. merged dataset)

```{r}
qda_mod <- discrim_quad() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")

# create workflow
qda_wkflow <- workflow() %>% 
  add_model(qda_mod) %>% 
  add_recipe(grades_recipe1_fctr) # recipe from before

# fit
qda_fit <- fit(qda_wkflow, grades_train_fctr)

# predict
predict(qda_fit, new_data = grades_train_fctr, type = "prob")

# confusion matrix
augment(qda_fit, new_data = grades_train_fctr) %>%
  conf_mat(truth = g3, estimate = .pred_class)

# accuracy and roc_auc
augment(qda_fit, new_data = grades_train_fctr) %>%
  accuracy(truth = g3, estimate = .pred_class)

augment(qda_fit, new_data = grades_train_fctr) %>%
  roc_auc(truth = g3, estimate = .pred_Fail)
```

#### Quadratic Discriminant Analysis (QDA) Model (Method 2. Portuguese for training, Maths for testing)

```{r}
qda_mod <- discrim_quad() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")

# create workflow
qda_wkflow2 <- workflow() %>% 
  add_model(qda_mod) %>% 
  add_recipe(grades_recipe2_fctr) # recipe from before

# fit
qda_fit2 <- fit(qda_wkflow2, grades_train2_fctr)

# predict
predict(qda_fit2, new_data = grades_train2_fctr, type = "prob")

# confusion matrix
augment(qda_fit2, new_data = grades_train2_fctr) %>%
  conf_mat(truth = g3, estimate = .pred_class)

# accuracy and roc
augment(qda_fit2, new_data = grades_train2_fctr) %>%
  accuracy(truth = g3, estimate = .pred_class)

augment(qda_fit2, new_data = grades_train2_fctr) %>%
  roc_auc(truth = g3, estimate = .pred_Fail)
```

From the three different models, like logistic regression, specifically Method 2, giving us the highest accuracy of 0.9322034. We also notice that for logistic regression and QDA, Method 2 (Portuguese=training, Maths=testing) of data splitting gave higher accuracy. But for QDA, Method 1 (merged dataset) of data splitting gave a higher accuracy.


### K-fold Cross Validation

We can try k-fold cross validation on the model that performed best out of classification models (logistic regression). Let's use 10 folds and try it on both the methods of data splitting. Will the Method 1. merged dataset or Method 2. Portuguese/Maths split dataset give us better results?

Before we jump in, first, some definitions. AUC is the "area under the curve." "Curve" refers to an ROC curve. AUC represents the model's capability of distinguishing between classes. In our scenario, AUC is measuring how capable each model is at distinguishing students who "Fail" from students who "Pass."
We will also be plotting ROC curves. This website's says "A ROC curve is constructed by plotting the true positive rate (TPR) against the false positive rate (FPR)." https://www.displayr.com/what-is-a-roc-curve-how-to-interpret-it/
Basically, an ROC curve measures a model's performance at classifying the data into the correct categories (ex. "Fail", "Pass")

#### Method 1. merged dataset

```{r, message = F, warning = F}
set.seed(3435)

# 10-fold cross validation
grades_folds_fctr <- vfold_cv(grades_train_fctr, v = 10)

# logistic regression model
log_reg <- logistic_reg() %>% 
  set_engine("glm")

# create workflow
log_wkflow <- workflow() %>% 
  add_model(log_reg) %>% 
  add_recipe(grades_recipe1_fctr)

# fit to the folds
fit_log_kfold <- fit_resamples(log_wkflow,
                        grades_folds_fctr,
                        control = control_resamples(save_pred = TRUE))

# fit model to entire training dataset
final_fit <- fit(log_wkflow, grades_train_fctr)

# bind predictions to actual observations
grades_predicted <- predict(final_fit, new_data = grades_train_fctr)
grades_predicted <- bind_cols(grades_predicted, grades_train_fctr %>% select(g3))
grades_predicted

# confusion matrix
augment(final_fit, new_data = grades_train_fctr) %>%
  conf_mat(truth = g3, estimate = .pred_class)

# accuracy and roc_auc
multi_metric <- metric_set(accuracy, sensitivity, specificity)
augment(final_fit, new_data = grades_train_fctr) %>%
  multi_metric(truth = g3, estimate = .pred_class)

augment(final_fit, new_data = grades_train_fctr) %>%
  roc_auc(truth = g3, estimate = .pred_Fail)

# ROC curve
augment(final_fit, new_data = grades_train_fctr) %>%
  roc_curve(truth = grades_train_fctr$g3, estimate = .pred_Fail) %>%
  autoplot()
```

#### Method 2. Portuguese for training, Maths for testing

```{r, message = F, warning = F}
set.seed(3435)

# 10-fold cross validation
grades_folds2_fctr <- vfold_cv(grades_train2_fctr, v = 10)

# logistic regression model
log_reg <- logistic_reg() %>% 
  set_engine("glm")

# create workflow
log_wkflow2 <- workflow() %>% 
  add_model(log_reg) %>% 
  add_recipe(grades_recipe2_fctr)

# fit model to folds
fit_log2_kfold <- fit_resamples(log_wkflow2,
                        grades_folds2_fctr,
                        control = control_resamples(save_pred = TRUE))

# fit model to entire training dataset
final_fit2 <- fit(log_wkflow2, grades_train2_fctr)

# bind predictions to actual observations
grades_predicted2 <- predict(final_fit2, new_data = grades_train2_fctr, type = "prob")
grades_predicted2 <- bind_cols(grades_predicted2, grades_train2_fctr %>% select(g3))
grades_predicted2

# confusion matrix
augment(final_fit2, new_data = grades_train2_fctr) %>%
  conf_mat(truth = g3, estimate = .pred_class)

# accuracy and roc_auc
augment(final_fit2, new_data = grades_train2_fctr) %>%
  multi_metric(truth = g3, estimate = .pred_class)

augment(final_fit2, new_data = grades_train2_fctr) %>%
  roc_auc(truth = g3, estimate = .pred_Fail)

# ROC curve Method 2
augment(final_fit2, new_data = grades_train2_fctr) %>%
  roc_curve(truth = grades_train2_fctr$g3, estimate = .pred_Fail) %>%
  autoplot()
```

The ROC curves of two models are extremely similar, with slight variation at the middle of the curve. If we compare the ROC_AUC values, we see that the value for Method 1 is slightly bigger than the value for method 2. This is consistent with our results from the accuracy metrics above.

### Elastic Net Tuning

Next, let's try the elastic net method, where we tune the parameters of mixture and penalty. This is an a model that uses an "in between" penalty term in comparison to ridge and lasso regression (where the penalty terms are 0 and as close to infinity as possible, respectively).
Again, we fit the elastic net model to both Method 1 and Method 2 data to see which gives us better results.

#### Method 1. merged dataset

First we prepare our data by splitting into 5-folds for cross validation. Then we set the engine and create a workflow for multinomial regression (which is used for the elastic net method). Like we mentioned before, we want to tune the mixture and penalty parameters to see which values give us the best results.

```{r}
set.seed(3435)

# 5-fold cross validation
grades_folds_fctr_new <- vfold_cv(grades_train_fctr, v = 5, strata = g3)

# elastic net
multinom_reg_model <- multinom_reg(mixture = tune(), penalty = tune()) %>% 
  set_mode("classification") %>%
  set_engine("glmnet")

# create workflow
multinom_reg_wf <- workflow() %>%
  add_model(multinom_reg_model) %>% # add model
  add_recipe(grades_recipe1_fctr) # add recipe

# grid for penalty and mixture
grid_pm_en <- grid_regular(mixture(range = c(0, 1)), penalty(range = c(-5, 5)), levels = c(mixture = 10, penalty = 10))
```

```{r, eval = F}
tune_res_elasticnet <- tune_grid(
  object = multinom_reg_wf, 
  resamples = grades_folds_fctr_new, 
  grid = grid_pm_en
)

# write file
write_rds(tune_res_elasticnet, file = "C:/Users/cupca/OneDrive/Documents/UCSB/Fall2022/PSTAT131/Final-Project/Final-Project-laurenrhuang/saved_models/tune_res_elasticnet.rds")
```

Models like these which have tuned parameters and involve k-fold cross validation can take longer to run, a good idea would be to save your fitted model to a file so you don't have to re-fit the model every time you run the code!

```{r}
# read file
tune_res_elasticnet <- read_rds(file = "C:/Users/cupca/OneDrive/Documents/UCSB/Fall2022/PSTAT131//Final-Project/Final-Project-laurenrhuang/saved_models/tune_res_elasticnet.rds")

# autoplot
autoplot(tune_res_elasticnet)
```

```{r}
# show the best models according to roc_auc
show_best(tune_res_elasticnet, metric = "roc_auc")

# select the best
en_best_rocauc <- select_best(tune_res_elasticnet, metric = "roc_auc")
en_best_rocauc

# fit model to training set
elasticnet1_final <- finalize_workflow(multinom_reg_wf, en_best_rocauc)
elasticnet1_final_fit <- fit(elasticnet1_final, data = grades_train_fctr)

# confusion matrix
augment(elasticnet1_final_fit, new_data = grades_train_fctr) %>%
  conf_mat(truth = g3, estimate = .pred_class) %>%
  autoplot(type = "heatmap")

# evaluate accuracy and roc_auc
augment(elasticnet1_final_fit, new_data = grades_train_fctr) %>%
  accuracy(truth = g3, estimate = .pred_class)

augment(elasticnet1_final_fit, new_data = grades_train_fctr) %>%
  roc_auc(truth = g3, estimate = .pred_Fail)

# ROC curve
augment(elasticnet1_final_fit, new_data = grades_train_fctr) %>%
  roc_curve(truth = g3, estimate = .pred_Fail) %>%
  autoplot()
```

#### Method 2. Portuguese for training, Maths for testing

```{r}
set.seed(3435)

# 5-fold cross validation
grades_folds2_fctr_new <- vfold_cv(grades_train2_fctr_nocourse, v = 5, strata = g3)

# elastic net
multinom_reg_model <- multinom_reg(mixture = tune(), penalty = tune()) %>% 
  set_mode("classification") %>%
  set_engine("glmnet")

# create workflow
multinom_reg_wf2 <- workflow() %>%
  add_model(multinom_reg_model) %>% # add model
  add_recipe(grades_recipe2_fctr) # add recipe
```

```{r, eval = F}
tune_res_elasticnet2 <- tune_grid(
  object = multinom_reg_wf2, 
  resamples = grades_folds2_fctr_new, 
  grid = grid_pm_en
)

# write file
write_rds(tune_res_elasticnet2, file = "C:/Users/cupca/OneDrive/Documents/UCSB/Fall2022/PSTAT131/Final-Project/Final-Project-laurenrhuang/saved_models/tune_res_elasticnet2.rds")
```

```{r}
# read file
tune_res_elasticnet2 <- read_rds(file = "C:/Users/cupca/OneDrive/Documents/UCSB/Fall2022/PSTAT131/Final-Project/Final-Project-laurenrhuang/saved_models/tune_res_elasticnet2.rds")

# autoplot
autoplot(tune_res_elasticnet2)
```

```{r}
# show the best models according to roc_auc
show_best(tune_res_elasticnet2, metric = "roc_auc")

# select the best
en_best_rocauc2 <- select_best(tune_res_elasticnet2, metric = "roc_auc")
en_best_rocauc2

# fit model to training set
elasticnet2_final <- finalize_workflow(multinom_reg_wf2, en_best_rocauc2)
elasticnet2_final_fit <- fit(elasticnet2_final, data = grades_train2_fctr_nocourse)

# confusion matrix
augment(elasticnet2_final_fit, new_data = grades_train2_fctr_nocourse) %>%
  conf_mat(truth = g3, estimate = .pred_class) %>%
  autoplot(type = "heatmap")

# evaluate accuracy and roc_auc
augment(elasticnet2_final_fit, new_data = grades_train2_fctr_nocourse) %>%
  accuracy(truth = g3, estimate = .pred_class)

augment(elasticnet2_final_fit, new_data = grades_train2_fctr_nocourse) %>%
  roc_auc(truth = g3, estimate = .pred_Fail)

# ROC curve
augment(elasticnet2_final_fit, new_data = grades_train2_fctr_nocourse) %>%
  roc_curve(truth = g3, estimate = .pred_Fail) %>%
  autoplot()
```

From the results above, Method 1 gives us better accuracy and roc_auc when fitted to the elastic net model. We can verify the details by observing the confusion matrix and plotting ROC curves for each method. Are there any other models that can give us even better results?

### Decision Tree

Next, we move to the tree model family. A decision tree is one type of tree model. You can think of a decision tree as a branching method that looks at every possible output for a specific input. Take a look at the visualization in the next code chunk.

#### Method 1. merged dataset

```{r, warning = F}
# decision tree engine
tree_spec <- decision_tree() %>%
  set_engine("rpart")
class_tree_spec <- tree_spec %>%
  set_mode("classification")

# fit model
class_tree_fit <- class_tree_spec %>%
  fit(g3 ~ ., data = grades_train_fctr)

# visualize
class_tree_fit %>%
  extract_fit_engine() %>%
  rpart.plot()
```
Like the previous models, we set an engine for the decision tree model. We also make a grid to tune the cost complexity parameter. Note, the cost complexity parameter is very important because it helps "prune" the tree. This means it reduces the size of the tree and keeps the tree from having "too many branches" or "leaf nodes." A tree that has too many branches or leaf nodes could lead to model overfitting.
Let's set up the engine and workflow for a decision tree. Here, we tune the cost complexity parameter, to see which value gives us the best results.

```{r}
# decision tree engine
tree_spec <- decision_tree() %>%
  set_engine("rpart") %>%
  set_mode("classification")

# create workflow
class_tree_wf <- workflow() %>%
  add_model(tree_spec %>% set_args(cost_complexity = tune())) %>%
  add_recipe(grades_recipe1_fctr)

# create grid
set.seed(3435)
param_grid <- grid_regular(cost_complexity(range = c(-3, -1)), levels = 10)
```

```{r, eval = F}
# fit with tune_grid
classtree_tune_res <- tune_grid(
  class_tree_wf, 
  resamples = grades_folds_fctr_new, 
  grid = param_grid, 
  metrics = metric_set(accuracy, roc_auc)
)

# write file
write_rds(classtree_tune_res, file = "C:/Users/cupca/OneDrive/Documents/UCSB/Fall2022/PSTAT131/Final-Project/Final-Project-laurenrhuang/saved_models/classtree_tune_res.rds")
```

Next, we plot a comparison of the cost-complexity parameter to accuracy and roc_auc. What do you notice? It looks like accuracy peaks and flattens at about 0.92 once we reach a cost complexity of about 0.02.

```{r}
# read file
classtree_tune_res <- read_rds(file = "C:/Users/cupca/OneDrive/Documents/UCSB/Fall2022/PSTAT131/Final-Project/Final-Project-laurenrhuang/saved_models/classtree_tune_res.rds")

# cost-complexity autoplot
autoplot(classtree_tune_res)
```

We can take a closer look at all the values of the cost complexity parameter, and choose the one that gives us the best results. This matches what we see in the plot above, so that's good.

```{r}
# show the best models according to roc_auc
show_best(classtree_tune_res, metric = "roc_auc")

# select the best
classtree_best_complexity <- select_best(classtree_tune_res, metric = "roc_auc")
classtree_best_complexity

# fit
classtree_final1 <- finalize_workflow(class_tree_wf, classtree_best_complexity)
classtree_final1_fit <- fit(classtree_final1, data = grades_train_fctr)

# evaluate accuracy and roc_auc
augment(classtree_final1_fit, new_data = grades_train_fctr) %>%
  accuracy(truth = g3, estimate = .pred_class)

augment(classtree_final1_fit, new_data = grades_train_fctr) %>%
  roc_auc(truth = g3, estimate = .pred_Fail)
```


#### Method 2. Portuguese for training, Maths for testing

```{r, warning = F}
# decision tree engine
tree_spec <- decision_tree() %>%
  set_engine("rpart")
class_tree_spec <- tree_spec %>%
  set_mode("classification")

# fit model
class_tree_fit2 <- class_tree_spec %>%
  fit(g3 ~ ., data = grades_train2_fctr)

# visualize
class_tree_fit %>%
  extract_fit_engine() %>%
  rpart.plot()
```

```{r}
# decision tree engine
tree_spec <- decision_tree() %>%
  set_engine("rpart") %>%
  set_mode("classification")

# create workflow
class_tree_wf2 <- workflow() %>%
  add_model(tree_spec %>% set_args(cost_complexity = tune())) %>%
  add_recipe(grades_recipe2_fctr)
```

```{r, eval = F}
# fit with tune_grid
classtree_tune_res2 <- tune_grid(
  class_tree_wf2, 
  resamples = grades_folds2_fctr_new, 
  grid = param_grid, 
  metrics = metric_set(accuracy, roc_auc)
)

# write file
write_rds(classtree_tune_res2, file = "C:/Users/cupca/OneDrive/Documents/UCSB/Fall2022/PSTAT131/Final-Project/Final-Project-laurenrhuang/saved_models/classtree_tune_res2.rds")
```

```{r}
# read file
classtree_tune_res2 <- read_rds(file = "C:/Users/cupca/OneDrive/Documents/UCSB/Fall2022/PSTAT131/Final-Project/Final-Project-laurenrhuang/saved_models/classtree_tune_res2.rds")

# cost-complexity autoplot
autoplot(classtree_tune_res2)
```
Now this plot looks very different from the one we got for Method 1. This time, it seems that the cost-complexity parameter increases, peaks, drops very steeply, increases a bit, then levels out again. Why do you think this is?
We look at all the values of the cost complexity parameter, and choose the one that gives us the best results. This matches what we see in the plot above.

```{r}
# show the best models according to roc_auc
show_best(classtree_tune_res2, metric = "roc_auc")

# select the best
classtree_best_complexity2 <- select_best(classtree_tune_res2, , metric = "roc_auc")
classtree_best_complexity2

# fit
classtree_final2 <- finalize_workflow(class_tree_wf2, classtree_best_complexity2)
classtree_final_fit2 <- fit(classtree_final2, data = grades_train2_fctr_nocourse)

# evaluate accuracy and roc_auc
augment(classtree_final_fit2, new_data = grades_train2_fctr_nocourse) %>%
  accuracy(truth = g3, estimate = .pred_class)

augment(classtree_final_fit2, new_data = grades_train2_fctr_nocourse) %>%
  roc_auc(truth = g3, estimate = .pred_Fail)
```

### Boosted Tree

Will a boosted tree model give us better results than a decision tree? A boosted tree is like a decision tree, but boosting means that each tree is dependent on prior trees. Boosting uses a group of decision tree models, and learns by fitting the residual of the trees before it. 
Technically we could also try a random forest model, which is also constructed using multiple decision trees. The difference between the two is that each tree in random forests are independent, whereas the trees in boosted tree models are dependent. But because random forests and boosted tree models involve multiple decision trees, they take a long time to run. For the sake of time, we'll just try out a boosted tree.

#### Method 1. merged dataset

First, specify the boosted tree engine, workflow, and create a grid to tune the number of trees. Tuning will help us find the number of trees that gives us the best results.

```{r}
# set boosted tree engine
xgb_spec <- boost_tree() %>%
  set_engine("xgboost") %>%
  set_mode("classification")

# create workflow
xgb_wf <- workflow() %>%
  add_model(xgb_spec %>% set_args(trees = tune())) %>%
  add_recipe(grades_recipe1_fctr)

# create grid
param_grid2 <- grid_regular(trees(range = c(10, 2000)),
                            levels = 10)
```

```{r, eval = F}
# fit using tune_grid
boostedtree_tune_res <- tune_grid(
  xgb_wf, 
  resamples = grades_folds_fctr_new, 
  grid = param_grid2, 
  metrics = metric_set(accuracy, roc_auc)
)

# write file
write_rds(boostedtree_tune_res, file = "C:/Users/cupca/OneDrive/Documents/UCSB/Fall2022/PSTAT131/Final-Project/Final-Project-laurenrhuang/saved_models/boostedtree_tune_res.rds")
```

Let's plot the accuracies. The plot matches our metrics results, showing us that accuracy peaks at 0.8972186 around 1115 trees. 

```{r}
boostedtree_tune_res <- read_rds(file = "C:/Users/cupca/OneDrive/Documents/UCSB/Fall2022/PSTAT131/Final-Project/Final-Project-laurenrhuang/saved_models/boostedtree_tune_res.rds")

# autoplot
autoplot(boostedtree_tune_res)
```

```{r}
# show the best models according to roc_auc
show_best(boostedtree_tune_res, metric = "roc_auc")

# select the best
boostedtree_best_complexity <- select_best(boostedtree_tune_res, metric = "roc_auc")
boostedtree_best_complexity

# fit
xgb_final1 <- finalize_workflow(xgb_wf, boostedtree_best_complexity)
xgb_final_fit1 <- fit(xgb_final1, data = grades_train_fctr)

# evaluate accuracy and roc_auc
augment(xgb_final_fit1, new_data = grades_train_fctr) %>%
  accuracy(truth = g3, estimate = .pred_class)

augment(xgb_final_fit1, new_data = grades_train_fctr) %>%
  roc_auc(truth = g3, estimate = .pred_Fail)
```


The highest accuracy we get is 0.8972186, but we get an ROC AUC of 1. Why do you think this is? It may be because the model is biased, performing well on one class, but has some false negatives as well.

#### Method 2. Portuguese for training, Maths for testing

Here we'll do the same for Method 2. 

```{r}
# set boosted tree engine
xgb_spec <- boost_tree() %>%
  set_engine("xgboost") %>%
  set_mode("classification")

# create workflow
xgb_wf2 <- workflow() %>%
  add_model(xgb_spec %>% set_args(trees = tune())) %>%
  add_recipe(grades_recipe2_fctr)

# create grid
param_grid2 <- grid_regular(trees(range = c(10, 2000)),
                            levels = 10)
```

```{r, eval = F}
# fit using tune_grid
boostedtree_tune_res2 <- tune_grid(
  xgb_wf2, 
  resamples = grades_folds2_fctr_new, 
  grid = param_grid2, 
  metrics = metric_set(accuracy, roc_auc)
)

# write file
write_rds(boostedtree_tune_res2, file = "C:/Users/cupca/OneDrive/Documents/UCSB/Fall2022/PSTAT131/Final-Project/Final-Project-laurenrhuang/saved_models/boostedtree_tune_res2.rds")
```

This time it looks like accuracy peaks at 0.8981952 with about 231 trees, which is consistent with our plot.

```{r}
boostedtree_tune_res2 <- read_rds(file = "C:/Users/cupca/OneDrive/Documents/UCSB/Fall2022/PSTAT131/Final-Project/Final-Project-laurenrhuang/saved_models/boostedtree_tune_res2.rds")

# autoplot
autoplot(boostedtree_tune_res2)
```

```{r}
# show the best models according to roc_auc
show_best(boostedtree_tune_res2, metric = "roc_auc")

# select the best
boostedtree_best_complexity2 <- select_best(boostedtree_tune_res2, metric = "roc_auc")
boostedtree_best_complexity2

# fit
xgb_final2 <- finalize_workflow(xgb_wf2, boostedtree_best_complexity2)
xgb_final_fit2 <- fit(xgb_final2, data = grades_train2_fctr_nocourse)

# evaluate accuracy and roc_auc
augment(xgb_final_fit2, new_data = grades_train2_fctr_nocourse) %>%
  accuracy(truth = g3, estimate = .pred_class)

augment(xgb_final_fit2, new_data = grades_train2_fctr_nocourse) %>%
  roc_auc(truth = g3, estimate = .pred_Fail)
```

Let us fit the boosted tree model to our testing data and see how it performs.

In review, after analyzing both of our boosted tree models, Method 2 of data splitting gave us a slightly higher accuracy of 0.8981952. Let us now fit the data to our Method 1, which had the higher roc_auc. We can observe the results by making a ROC curve and heatmap of the confusion matrix to see if the results line up with our ROC AUC value.

```{r}
# select the best (boosted tree)
best_rocauc <- select_best(boostedtree_tune_res, metric = "roc_auc")

# fit
rocauc_final <- finalize_workflow(xgb_wf, boostedtree_best_complexity)
rocauc_final_fit <- fit(rocauc_final, data = grades_train_fctr)

# roc_auc
augment(xgb_final_fit1, new_data = grades_train_fctr) %>%
  roc_auc(truth = g3, estimate = .pred_Fail) %>%
  select(.estimate)

# roc curve
augment(rocauc_final_fit, new_data = grades_train_fctr) %>%
  roc_curve(truth = g3, estimate = .pred_Fail) %>%
  autoplot()

# confusion matrix
augment(rocauc_final_fit, new_data = grades_train_fctr) %>%
  conf_mat(truth = g3, estimate = .pred_class) %>%
  autoplot(type = "heatmap")
```

## Conclusion

After testing both regression and classification methods, it seems like most of our classification models performed better than the regression models.

```{r}
# Logistic Regression
grades_log_reg_auc <- augment(log_fit, new_data = grades_train_fctr) %>%
  roc_auc(truth = g3, estimate = .pred_Fail) %>%
  select(.estimate)

grades_log_reg_auc2 <- augment(log_fit2, new_data = grades_train2_fctr) %>%
  roc_auc(truth = g3, estimate = .pred_Fail) %>%
  select(.estimate)

# LDA
grades_lda_auc <- augment(lda_fit, new_data = grades_train_fctr) %>%
  roc_auc(truth = g3, estimate = .pred_Fail) %>%
  select(.estimate)

grades_lda_auc2 <- augment(lda_fit2, new_data = grades_train2_fctr) %>%
  roc_auc(truth = g3, estimate = .pred_Fail) %>%
  select(.estimate)

# QDA
grades_qda_auc <- augment(qda_fit, new_data = grades_train_fctr) %>%
  roc_auc(truth = g3, estimate = .pred_Fail) %>%
  select(.estimate)

grades_qda_auc2 <- augment(qda_fit2, new_data = grades_train2_fctr) %>%
  roc_auc(truth = g3, estimate = .pred_Fail) %>%
  select(.estimate)

# Log K-Fold CV
grades_log_kfold_auc <- augment(final_fit, new_data = grades_train_fctr) %>%
  roc_auc(truth = g3, estimate = .pred_Fail) %>%
  select(.estimate)

grades_log_kfold_auc2 <- augment(final_fit2, new_data = grades_train2_fctr) %>%
  roc_auc(truth = g3, estimate = .pred_Fail) %>%
  select(.estimate)

# Elastic Net
grades_elasticnet_auc <- augment(elasticnet1_final_fit, new_data = grades_train_fctr) %>%
  roc_auc(truth = g3, estimate = .pred_Fail) %>%
  select(.estimate)

grades_elasticnet_auc2 <- augment(elasticnet2_final_fit, new_data = grades_train2_fctr_nocourse) %>%
  roc_auc(truth = g3, estimate = .pred_Fail) %>%
  select(.estimate)

# Decision Tree
grades_dt_auc <- augment(classtree_final1_fit, new_data = grades_train_fctr) %>%
  roc_auc(truth = g3, estimate = .pred_Fail) %>%
  select(.estimate)

grades_dt_auc2 <- augment(classtree_final_fit2, new_data = grades_train2_fctr_nocourse) %>%
  roc_auc(truth = g3, estimate = .pred_Fail) %>%
  select(.estimate)

# Boosted Tree
grades_xgb_auc <- augment(xgb_final_fit1, new_data = grades_train_fctr) %>%
  roc_auc(truth = g3, estimate = .pred_Fail) %>%
  select(.estimate)

grades_xgb_auc2 <- augment(xgb_final_fit2, new_data = grades_train2_fctr_nocourse) %>%
  roc_auc(truth = g3, estimate = .pred_Fail) %>%
  select(.estimate)

grades_roc_aucs <- c(grades_log_reg_auc$.estimate,
                           grades_log_reg_auc2$.estimate,
                           grades_lda_auc$.estimate,
                           grades_lda_auc2$.estimate,
                           grades_qda_auc$.estimate,
                           grades_qda_auc2$.estimate,
                           grades_log_kfold_auc$.estimate,
                           grades_log_kfold_auc2$.estimate,
                           grades_elasticnet_auc$.estimate,
                           grades_elasticnet_auc2$.estimate,
                           grades_dt_auc$.estimate,
                           grades_dt_auc2$.estimate,  
                           grades_xgb_auc$.estimate,
                           grades_xgb_auc2$.estimate)

grades_mod_names <- c("Log Regression Method 1",
                      "Log Regression Method 2",
                      "LDA Method 1",
                      "LDA Method 2",
                      "QDA Method 1",
                      "QDA Method 2",
                      "Log Regression K-Fold CV Method 1",
                      "Log Regression K-Fold CV Method 2",
                      "Elastic Net Method 1",
                      "Elastic Net Method 2",
                      "Decision Tree Method 1",
                      "Decision Tree Method 2",
                      "Boosted Tree Method 1",
                      "Boosted Tree Method 2")
```

```{r}
grades_results <- tibble(Model = grades_mod_names,
                             ROC_AUC = grades_roc_aucs)

grades_results <- grades_results %>% 
  arrange(-grades_roc_aucs)

grades_results
```

These here are our classification models. We can make a dot plot to visualize these results.

```{r}
grades_dot_plot <- ggplot(grades_results, aes(x = Model, y = ROC_AUC)) +
  geom_point(fill = "#FB4F14", col = "#FB4F14", size=10) + 
  geom_segment(aes(x = Model, 
                   xend = Model, 
                   y=min(ROC_AUC), 
                   yend = max(ROC_AUC)), 
               linetype = "dashed", 
               linewidth=0.5) + 
  labs(title = "Model Performance") + 
  theme_minimal() +
  coord_flip()
```

```{r}
grades_dot_plot
```

Based on ROC_AUC values, it looks like the Boosted Tree using Method 1. merged dataset gave us the best results. Let's try fitting the testing data to this model.

```{r}
# show metrics
collect_metrics(boostedtree_tune_res) %>% arrange(desc(mean))

# select the best
boostedtree_best_complexity <- select_best(boostedtree_tune_res, metric = "roc_auc")
boostedtree_best_complexity

show_best(boostedtree_tune_res, metric = "roc_auc") %>% #showing the best rf model
  select(-.estimator, .config) %>%
  slice(1)

# fit model to testing data
grades_predict <- predict(xgb_final_fit1,
                          new_data = grades_test_fctr, 
                          type = "class")

# adding the actual values side by side to our predicted values
grades_predict_with_actual <- grades_predict %>%
  bind_cols(grades_test_fctr %>% select(g3))
grades_predict_with_actual

# computing the ROC curve for this model
grades_roc_curve <- augment(xgb_final_fit1, new_data = grades_test_fctr) %>%
  roc_curve(truth = g3, estimate = .pred_Fail)
autoplot(grades_roc_curve)

# computing the AUC for the ROC curve
finalgrades_roc_auc <- augment(xgb_final_fit1, new_data = grades_test_fctr) %>%
  roc_auc(truth = g3, estimate = .pred_Fail) %>%
  select(.estimate)
finalgrades_roc_auc
```

